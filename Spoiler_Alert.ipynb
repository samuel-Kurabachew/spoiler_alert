{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spoiler_Alert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tPoTVJedeNE"
      },
      "source": [
        "# Spoiler Alert.\n",
        "\n",
        "This notebook comprises of techniques and analysis steps we used to tackle the problem of spoiler in user reviews. We have approached it in two ways:\n",
        "1. Classification using simple traditional machine learning algorithms.\n",
        "2. State of the art algorithm, Deep Learning.\n",
        "\n",
        "The dataset can be found in kaggle following this link: [Dataset](https://www.kaggle.com/rmisra/imdb-spoiler-dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "qipd8poP7E9W",
        "outputId": "158b5428-8f77-47f8-f11b-f08d954d6648"
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\")) # Increase cell width\n",
        "display(HTML(\"<style>.rendered_html { font-size: 16px; }</style>\")) # Increase font size\n",
        "\n",
        "# Matplotlib conf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Seaborn conf\n",
        "import seaborn as sns\n",
        "sns.set_palette(sns.color_palette(\"seismic\"))\n",
        "\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import operator\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "import pandas \n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.rendered_html { font-size: 16px; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIIuigIO79EX",
        "outputId": "6e5e48a5-f13a-444e-dba8-49b90d3c497d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpyzsjhE8Cop"
      },
      "source": [
        "path = '/content/drive/My Drive/IMDB_spoiler_dataset/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFAPu5iPfqyF"
      },
      "source": [
        "# Insert the directory to import custom modules\n",
        "import sys\n",
        "sys.path.insert(0,'/content/drive/My Drive/Colab Notebooks/nlp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXIVuD4f8J0v"
      },
      "source": [
        "train_data = pd.read_json(path + 'IMDB_reviews.json', lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "7pKHREsD8pWl",
        "outputId": "9d2c6651-5004-4d36-9220-a6f28770a66e"
      },
      "source": [
        "print(train_data.shape)\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(573913, 7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_date</th>\n",
              "      <th>movie_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>is_spoiler</th>\n",
              "      <th>review_text</th>\n",
              "      <th>rating</th>\n",
              "      <th>review_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10 February 2006</td>\n",
              "      <td>tt0111161</td>\n",
              "      <td>ur1898687</td>\n",
              "      <td>True</td>\n",
              "      <td>In its Oscar year, Shawshank Redemption (writt...</td>\n",
              "      <td>10</td>\n",
              "      <td>A classic piece of unforgettable film-making.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6 September 2000</td>\n",
              "      <td>tt0111161</td>\n",
              "      <td>ur0842118</td>\n",
              "      <td>True</td>\n",
              "      <td>The Shawshank Redemption is without a doubt on...</td>\n",
              "      <td>10</td>\n",
              "      <td>Simply amazing. The best film of the 90's.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3 August 2001</td>\n",
              "      <td>tt0111161</td>\n",
              "      <td>ur1285640</td>\n",
              "      <td>True</td>\n",
              "      <td>I believe that this film is the best story eve...</td>\n",
              "      <td>8</td>\n",
              "      <td>The best story ever told on film</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1 September 2002</td>\n",
              "      <td>tt0111161</td>\n",
              "      <td>ur1003471</td>\n",
              "      <td>True</td>\n",
              "      <td>**Yes, there are SPOILERS here**This film has ...</td>\n",
              "      <td>10</td>\n",
              "      <td>Busy dying or busy living?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20 May 2004</td>\n",
              "      <td>tt0111161</td>\n",
              "      <td>ur0226855</td>\n",
              "      <td>True</td>\n",
              "      <td>At the heart of this extraordinary movie is a ...</td>\n",
              "      <td>8</td>\n",
              "      <td>Great story, wondrously told and acted</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        review_date  ...                                 review_summary\n",
              "0  10 February 2006  ...  A classic piece of unforgettable film-making.\n",
              "1  6 September 2000  ...     Simply amazing. The best film of the 90's.\n",
              "2     3 August 2001  ...               The best story ever told on film\n",
              "3  1 September 2002  ...                     Busy dying or busy living?\n",
              "4       20 May 2004  ...         Great story, wondrously told and acted\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xpOwqgl_FP_",
        "outputId": "d2a4358d-ce35-4626-8ec4-16e3d915829e"
      },
      "source": [
        "print(train_data[\"review_text\"][0])\n",
        "print(\"======================== IS SPOILER ===================\")\n",
        "print(train_data[\"is_spoiler\"][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In its Oscar year, Shawshank Redemption (written and directed by Frank Darabont, after the novella Rita Hayworth and the Shawshank Redemption, by Stephen King) was nominated for seven Academy Awards, and walked away with zero. Best Picture went to Forrest Gump, while Shawshank and Pulp Fiction were \"just happy to be nominated.\" Of course hindsight is 20/20, but while history looks back on Gump as a good film, Pulp and Redemption are remembered as some of the all-time best. Pulp, however, was a success from the word \"go,\" making a huge splash at Cannes and making its writer-director an American master after only two films. For Andy Dufresne and Co., success didn't come easy. Fortunately, failure wasn't a life sentence.After opening on 33 screens with take of $727,327, the $25M film fell fast from theatres and finished with a mere $28.3M. The reasons for failure are many. Firstly, the title is a clunker. While iconic to fans today, in 1994, people knew not and cared not what a 'Shawshank' was. On the DVD, Tim Robbins laughs recounting fans congratulating him on \"that 'Rickshaw' movie.\" Marketing-wise, the film's a nightmare, as 'prison drama' is a tough sell to women, and the story of love between two best friends doesn't spell winner to men. Worst of all, the movie is slow as molasses. As Desson Thomson writes for the Washington Post, \"it wanders down subplots at every opportunity and ignores an abundance of narrative exit points before settling on its finale.\" But it is these same weaknesses that make the film so strong.Firstly, its setting. The opening aerial shots of the prison are a total eye-opener. This is an amazing piece of architecture, strong and Gothic in design. Immediately, the prison becomes a character. It casts its shadow over most of the film, its tall stone walls stretching above every shot. It towers over the men it contains, blotting out all memories of the outside world. Only Andy (Robbins) holds onto hope. It's in music, it's in the sandy beaches of Zihuatanejo; \"In here's where you need it most,\" he says. \"You need it so you don't forget. Forget that there are places in the world that aren't made out of stone. That there's a - there's a - there's something inside that's yours, that they can't touch.\" Red (Morgan Freeman) doesn't think much of Andy at first, picking \"that tall glass o' milk with the silver spoon up his ass\" as the first new fish to crack. Andy says not a word, and losing his bet, Red resents him for it. But over time, as the two get to know each other, they quickly become the best of friends. This again, is one of the film's major strengths. Many movies are about love, many flicks have a side-kick to the hero, but Shawshank is the only one I can think of that looks honestly at the love between two best friends. It seems odd that Hollywood would skip this relationship time and again, when it's a feeling that weighs so much into everyone's day to day lives. Perhaps it's too sentimental to seem conventional, but Shawshank's core friendship hits all the right notes, and the film is much better for it.It's pacing is deliberate as well. As we spend the film watching the same actors, it is easy to forget that the movie's timeline spans well over 20 years. Such a huge measure of time would pass slowly in reality, and would only be amplified in prison. And it's not as if the film lacks interest in these moments. It still knows where it's going, it merely intends on taking its sweet time getting there. It pays off as well, as the tedium of prison life makes the climax that much more exhilarating. For anyone who sees it, it is a moment never to be forgotten.With themes of faith and hope, there is a definite religious subtext to be found here. Quiet, selfless and carefree, Andy is an obvious Christ figure. Warden Norton (Bob Gunton) is obviously modeled on Richard Nixon, who, in his day, was as close to a personified Satan as they come. But if you aren't looking for subtexts, the movie speaks to anyone in search of hope. It is a compelling drama, and a very moving film, perfectly written, acted and shot. They just don't come much better than this.OVERALL SCORE: 9.8/10 = A+ The Shawshank Redemption served as a message of hope to Hollywood as well. More than any film in memory, it proved there is life after box office. Besting Forrest and Fiction, it ran solely on strong word of mouth and became the hottest rented film of 1995. It currently sits at #2 in the IMDb's Top 250 Films, occasionally swapping spots with The Godfather as the top ranked film of all time -- redemption indeed. If you haven't seen it yet, what the hell are you waiting for? As Andy says, \"It comes down a simple choice, really. Either get busy living, or get busy dying.\"\n",
            "======================== IS SPOILER ===================\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "VqneOhs2_LDs",
        "outputId": "de548ea7-7d68-4c19-f737-626655690946"
      },
      "source": [
        "sns.countplot(x=train_data.is_spoiler, order=[x for x, count in sorted(Counter(train_data.is_spoiler).items(), key=lambda x: -x[1])], palette=\"seismic\")\n",
        "plt.xticks(rotation=90);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM-9uVtneaA2"
      },
      "source": [
        "Now as we can see above, we have more than `500000` reviews yet we have an imbalanced dataset. This creates a problem specifically in classification tasks called **Accuracy Paradox**. This is the case when a simple model may have a high level of accuracy but be too crude to be useful. For example, if the incidence of category A is dominant, being found in 99% of cases, then predicting that every case is category A will have an accuracy of 99%; which translates to our model might flag reviews as **non-spoilers** most of the time.\n",
        "\n",
        "To take care of class imbalance we have different techniques:\n",
        "- We can look for different datasets with similar data and append the data of lower class with the additional data from different datasets in order to make it more balanced. \n",
        "\n",
        "- Balance the data using oversampling technique\n",
        "\n",
        "- While splitting Train and Test data, we can give the stratify parameter the output column that we are trying to predict, so that they show a balanced distribution on the train and test set\n",
        "\n",
        "- We can also give `class_weights` as a parameter to our model's fit method\n",
        "\n",
        "The solution we went with was having some threshold and get an eqaul balance of both our cases, spoiler and non-spoiler. Apart from solving the class imbalance issue, the reason we chose this was, when we choose the stratifying method and used the whole dataset, preprocessing and training times were super long which took most of our time working on the project. For this reason we minimized our dataset size to 200000 reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G_H4ITBzkqy"
      },
      "source": [
        "spoiler_data = train_data[train_data[\"is_spoiler\"]==True][:100000]\n",
        "non_spoiler_data = train_data[train_data[\"is_spoiler\"]==False][:100000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S66prh5dzr7i"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Combine spoiler and non_spoiler dataset\n",
        "all_data = pd.concat([spoiler_data, non_spoiler_data], ignore_index=True)\n",
        "\n",
        "# Shuffle data\n",
        "all_data = all_data.sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "PPhOgnfTxaa_",
        "outputId": "f1370170-31fe-41e1-b410-a11469d8cd54"
      },
      "source": [
        "sns.countplot(x=all_data.is_spoiler, order=[x for x, count in sorted(Counter(all_data.is_spoiler).items(), key=lambda x: -x[1])], palette=\"seismic\")\n",
        "plt.xticks(rotation=90);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxfvtm8wkktS"
      },
      "source": [
        "Perfect, now that we have a balanced and adequate amount of training corpus. The next step is preprocessing and some analysis on our reviews. \n",
        "\n",
        "In one of our forums we have seen that, by plotting the most repeated words in each class we expected to see the words \"most representative\" of each class. If they were different enough, this will indicate us that we can use them to easily to identify the classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuz0MZzZHti9"
      },
      "source": [
        "def plot_most_common_features(text_col, target_col, n_features=50):\n",
        "    from matplotlib import interactive\n",
        "\n",
        "    df = pd.DataFrame({\"review_text\": text_col, \"CLASS\": target_col})\n",
        "    grouped = df.groupby([\"CLASS\"]).apply(lambda x: x[\"review_text\"].sum())\n",
        "    grouped_df = pd.DataFrame({\"CLASS\": grouped.index, \"review_text\": grouped.values})\n",
        "\n",
        "    from nltk.tokenize import WhitespaceTokenizer\n",
        "    tokenizer = WhitespaceTokenizer()\n",
        "\n",
        "    for ii, text in enumerate(grouped_df.review_text):\n",
        "        pd.DataFrame(tokenizer.tokenize(text)).apply(pd.value_counts).head(n_features).plot(kind=\"bar\", cmap=plt.cm.seismic, figsize=(20,5))\n",
        "        plt.title(grouped_df.CLASS[ii], fontsize=20)\n",
        "        plt.xticks(fontsize=15)\n",
        "        plt.legend([])\n",
        "        interactive(True)\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUI3tz6jHmP-"
      },
      "source": [
        "from nltk.stem import *\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import re\n",
        "\n",
        "def process_text(raw_text):\n",
        "\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \",raw_text) \n",
        "    words = letters_only.lower().split()\n",
        "    \n",
        "    stops = set(stopwords.words(\"english\")) \n",
        "    not_stop_words = [w for w in words if not w in stops]\n",
        "    \n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed = [stemmer.stem(word) for word in not_stop_words]\n",
        "    \n",
        "    return( \" \".join( stemmed ))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UUTSxP5HoY8"
      },
      "source": [
        "all_data['clean_text'] = all_data['review_text'].apply(lambda x: process_text(x))\n",
        "plot_most_common_features(all_data.clean_text, all_data.is_spoiler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApXKZvdulEai"
      },
      "source": [
        "Looks like we have many common words and it's a bit vague to see which words are the most representative of our classes. We also know that we need to take care of special characters and punctuations in our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T09qImqobT7g"
      },
      "source": [
        "def text_to_word_list(text):\n",
        "    ''' Pre process and convert texts to a list of words '''\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!:.\\/'+-=]\", \" \", text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo_FJY_lbYPR"
      },
      "source": [
        "def process_text(raw_text):\n",
        "    \n",
        "    # raw_text = text_to_word_list(raw_text)\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \",raw_text) \n",
        "    words = letters_only.lower().split()\n",
        "    \n",
        "    stops = set(stopwords.words(\"english\"))  \n",
        "    not_stop_words = [w for w in words if not w in stops]\n",
        "    \n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed = [stemmer.stem(word) for word in not_stop_words]\n",
        "    \n",
        "    return( \" \".join( stemmed )) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9hXSAQhbZ7A"
      },
      "source": [
        "all_data['clean_text'] = all_data['review_text'].apply(lambda x: process_text(x))\n",
        "plot_most_common_features(all_data.clean_text, all_data.is_spoiler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAHcE64ald7Z"
      },
      "source": [
        "##  Train Test Split.\n",
        "\n",
        "We have seen earlier that most of the words are similar, but we can understand if a review is a spoiler or not depending on it's context and information contained. Can this intelligence refelected by our ML models? Let's see. We'll start by splitting our data to train and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EM_SNS7I7PG"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_data[[\"clean_text\"]], all_data[[\"is_spoiler\"]],\n",
        "                                                    stratify=all_data[[\"is_spoiler\"]], \n",
        "                                                    test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riXqtE3XZEMD"
      },
      "source": [
        "# Hot encoding for the labels\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(y_train.is_spoiler.values)\n",
        "target_labels = le.classes_\n",
        "encoded_y_train = le.transform(y_train.is_spoiler.values)\n",
        "encoded_y_test = le.transform(y_test.is_spoiler.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CVKg37siyex"
      },
      "source": [
        "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPwR3L22iz-S",
        "outputId": "6bd6d377-c2cc-4539-f2fe-515a7865b2e9"
      },
      "source": [
        "le_name_mapping"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{False: 0, True: 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLsr4mqQZbxa"
      },
      "source": [
        "count_vect = CountVectorizer(analyzer = \"word\")\n",
        "train_features = count_vect.fit_transform(X_train['clean_text'])\n",
        "test_features = count_vect.transform(X_test['clean_text'])\n",
        "\n",
        "tfidf = TfidfTransformer(norm=\"l2\")\n",
        "train_text_tfidf_features = tfidf.fit_transform(train_features)\n",
        "test_text_tfidf_features = tfidf.fit_transform(test_features)      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eektI8ehZfa5"
      },
      "source": [
        "def train_and_evaluate_classifier(X, yt, estimator, grid):\n",
        "    \"\"\"Train and Evaluate a estimator (defined as input parameter) on the given labeled data using accuracy.\"\"\"\n",
        "    \n",
        "    # Cross validation\n",
        "    from sklearn.model_selection import ShuffleSplit\n",
        "    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
        "      \n",
        "    from sklearn.model_selection import GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=estimator, cv=cv,  param_grid=grid, error_score=0.0, n_jobs = -1, verbose = 0)\n",
        "    \n",
        "    # Train the model over and tune the parameters\n",
        "    print(\"Training model\")\n",
        "    grid_search.fit(X, yt)\n",
        "\n",
        "    # CV-score\n",
        "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
        "    if len(grid) > 0:\n",
        "      best_parameters = grid_search.best_estimator_.get_params()\n",
        "      print(\"Best parameters set: \")\n",
        "      print(best_parameters)\n",
        "        \n",
        "\n",
        "    return grid_search"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffqNP07GmHWj"
      },
      "source": [
        "## Naïve Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGUSRmy5ZjfU",
        "outputId": "8a3282d7-be62-41db-f07d-dd5ace6d3088"
      },
      "source": [
        "nb_text_cls = train_and_evaluate_classifier(train_text_tfidf_features, encoded_y_train, MultinomialNB(), {})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model\n",
            "Best score: 0.808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_zW_QwGaZ4g",
        "outputId": "b4baef60-65cc-4a02-bb56-15acd309a33e"
      },
      "source": [
        "nb_text_cls.score(test_text_tfidf_features, encoded_y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.804"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YgPmtn2mMqm"
      },
      "source": [
        "Not bad! We have best score of 80% on the test data. Let's see how our model performs when we give it a direct inpute from the user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW6wTtIyac7W"
      },
      "source": [
        "def predict(text, model):\n",
        "  text_feature = count_vect.transform(text)\n",
        "  text_tfidf_feature = tfidf.fit_transform(text_feature)\n",
        "  \n",
        "  pred = model.predict(text_tfidf_feature)\n",
        "  # print custom response\n",
        "  if (pred.item()==1):\n",
        "      print(\"Spoiler!\")\n",
        "  elif (pred.item()==0):\n",
        "      print(\"Not Spoiler!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqp5kJ6fSgIj"
      },
      "source": [
        "text = \"Spoiler Alert! probabl best action film time ingredi consid best best first back ultim movi spectacular f x action sequenc nowaday movi look date still manag shock surreal nightmarish post apocalyt set monstruou robot action f x great great best thing movi termin spend lot effort fantast plot charact develop need extrem use f x charact interest background realli get plot simpli great futurist tale involv end world hand technolog time war human machin action non stop let want enough arnold co spectacular gun amaz explos memor chase sequenc freeway motorcycl etc movi rank high fun factor entertain factor arnold schwarzenegg deliv kickass perform everi time say hasta la vista babi problema trademark line skin crawl part pop cultur fit perfect movi feel linda hamilton sexi hell deliv brave solid perform special mention edward furlong deliv great perform robert patrick steal show villain impress jame cameron creat one best movi time bring us best possibl action ever seen movi also room gloriou gore non stop violenc els much say truli recommend everyon one movi chang point view toward cinema\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND6L381ziZEz",
        "outputId": "37d8c4c7-f489-40ee-a402-ff70d880587f"
      },
      "source": [
        "predict([text], nb_text_cls)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spoiler!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIRIPrTzma8O"
      },
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBHG5BOsigwO",
        "outputId": "59dceb81-c2c6-46ec-a048-fe6c84d8a348"
      },
      "source": [
        "# LR model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_text_cls = train_and_evaluate_classifier(train_text_tfidf_features, encoded_y_train, LogisticRegression(random_state=42), {})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model\n",
            "Best score: 0.809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skcN-R1vTlx8"
      },
      "source": [
        "text = \"This is spoiler review! Jon Snow kills dany in the end season of GOT\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1gn2CqoklQY",
        "outputId": "f905f6dc-00c8-4856-f239-5e237ec81e46"
      },
      "source": [
        "predict([text], lr_text_cls)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spoiler!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcjrrfQwZ-VU",
        "outputId": "28071e10-43fb-4c92-8731-d8d6908bc8fe"
      },
      "source": [
        "lr_text_cls.score(test_text_tfidf_features, encoded_y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.80825"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnJJdoHmmgxR"
      },
      "source": [
        "It's obvious that our two models give comparable results. Not let's move on to a more sophisticated solution using deep learning. This will be also the model we'll try to deploy in aws for our web app."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upNYRP_nZv5a"
      },
      "source": [
        "# Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9C3C0ILW3iB"
      },
      "source": [
        "Now that we have seen a simple yet robust classification algorithms, it's time to use state of the art technology specifically tailored for this kind of problems, Recurrent Neural Network (RNN). **Recurrent Neural Network** is a generalization of feedforward neural network that has an internal memory. RNN is recurrent in nature as it performs the same function for every input of data while the output of the current input depends on the past one computation. After producing the output, it is copied and sent back into the recurrent network. For making a decision, it considers the current input and the output that it has learned from the previous input. The temporal dependency of RNNs is what makes them ideal in langauage processing specially LSTMs. **Long Short-Term Memory (LSTM)** networks are a modified version of recurrent neural networks, which makes it easier to remember past data in memory. The vanishing gradient problem of RNN is also resolved here. LSTM is well-suited to classify, process and predict time series given time lags of unknown duration. \n",
        "\n",
        "Now we have a little bit background let's dive into the fun part. :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1f6HjsSfFj2"
      },
      "source": [
        "import os\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = all_data['review_text']\n",
        "y = list(map(lambda x: 0 if x==False else 1, all_data['is_spoiler']))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpqxPYctai2_",
        "outputId": "0ecc36ce-1ecd-4de9-875a-8800a924d0ca"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59512     I'm 18 years old now and Tolkien's Middle-Eart...\n",
              "97693     This movie is unbelievably bad or, rather, sup...\n",
              "175009    The third and last part of the Bourne trilogy ...\n",
              "149870    This movie is spectacular. Chinatown has one o...\n",
              "193689    Summer blockbuster season always brings with i...\n",
              "Name: review_text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-zBhwTbWsVi"
      },
      "source": [
        "The first step in processing the reviews is to make sure that any symbols or tags that appear should be removed. In addition we wish to tokenize our input, that way words such as *entertained* and *entertaining* are considered the same with regard to sentiment analysis. The `review_to_words` method defined above uses the nltk package to tokenize the reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlXjtar0alES"
      },
      "source": [
        "from nltk.stem.porter import *\n",
        "\n",
        "\n",
        "def review_to_words(review):\n",
        "  stemmer = PorterStemmer()\n",
        "\n",
        "  text = str(review)\n",
        "  text = text.lower()\n",
        "\n",
        "  text = re.sub(r\"[^A-Za-z0-9^,!:.\\/'+-=]\", \" \", text)\n",
        "\n",
        "  words = text.split()\n",
        "  words = [w for w in words if w not in stopwords.words(\"english\")]\n",
        "  words = [PorterStemmer().stem(w) for w in words]\n",
        "\n",
        "  return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nEUR4WwWxYM"
      },
      "source": [
        "\n",
        "The method below applies the `review_to_words` method to each of the reviews in the training and testing datasets. In addition it caches the results. This is because performing this processing step can take a long time. This way if you are unable to complete the notebook in the current session, you can come back without needing to process the data a second time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ2r2jg1bY5W"
      },
      "source": [
        "import pickle\n",
        "\n",
        "cache_dir = os.path.join(\"/content/drive/My Drive/Colab Notebooks/nlp/cache\", \"sentiment_analysis\")  # where to store cache files\n",
        "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
        "\n",
        "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
        "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
        "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
        "\n",
        "    # If cache_file is not None, try to read from it first\n",
        "    cache_data = None\n",
        "    if cache_file is not None:\n",
        "        try:\n",
        "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
        "                cache_data = pickle.load(f)\n",
        "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
        "        except:\n",
        "            pass  # unable to read from cache, but that's okay\n",
        "    \n",
        "    # If cache is missing, then do the heavy lifting\n",
        "    if cache_data is None:\n",
        "        # Preprocess training and test data to obtain words for each review\n",
        "        #words_train = list(map(review_to_words, data_train))\n",
        "        #words_test = list(map(review_to_words, data_test))\n",
        "        words_train = [review_to_words(review) for review in data_train]\n",
        "        words_test = [review_to_words(review) for review in data_test]\n",
        "        \n",
        "        # Write to cache file for future runs\n",
        "        if cache_file is not None:\n",
        "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
        "                              labels_train=labels_train, labels_test=labels_test)\n",
        "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
        "                pickle.dump(cache_data, f)\n",
        "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
        "    else:\n",
        "        # Unpack data loaded from cache file\n",
        "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
        "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
        "    \n",
        "    return words_train, words_test, labels_train, labels_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iPOyeBZblR0",
        "outputId": "21173a1d-66c8-4adb-9346-a97a0a85a40a"
      },
      "source": [
        "# Preprocess data\n",
        "X_train, X_test, y_train, y_test = preprocess_data(X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read preprocessed data from cache file: preprocessed_data.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyVMA_ybZrLB"
      },
      "source": [
        "## Transform the data\n",
        "\n",
        "For the next step in this notebook we will construct a feature representation called bag-of-words. To start, we will represent each word as an integer. Of course, some of the words that appear in the reviews occur very infrequently and so likely don't contain much information for the purposes of the review analysis. The way we will deal with this problem is that we will fix the size of our working vocabulary and we will only include the words that appear most frequently. We will then combine all of the infrequent words into a single category and, in our case, we will label it as 1.\n",
        "\n",
        "Since we will be using a recurrent neural network, it will be convenient if the length of each review is the same. To do this, we will fix a size for our reviews and then pad short reviews with the category 'no word' (which we will label 0) and truncate long reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBUCU2kYZ_2Y"
      },
      "source": [
        "To begin with, we need to construct a way to map words that appear in the reviews to integers. Here we fix the size of our vocabulary (including the 'no word' and 'infrequent' categories) to be `5000` but you may wish to change this to see how it affects the model.\n",
        "\n",
        " Note that even though the vocab_size is set to 5000, we only want to construct a mapping for the most frequently appearing 4998 words. This is because we want to reserve the special labels `0` for 'no word' and `1` for 'infrequent word'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRtx_m5JbrkD"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def build_dict(data, vocab_size = 5000):\n",
        "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
        "    \n",
        "    # Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a\n",
        "    # sentence is a list of words.\n",
        "    \n",
        "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
        "    \n",
        "    for review in data:\n",
        "        for word in review:\n",
        "            if word in word_count:\n",
        "                word_count[word] += 1\n",
        "            else:\n",
        "                word_count[word] = 1\n",
        "    # Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
        "    # sorted_words[-1] is the least frequently appearing word.\n",
        "    \n",
        "    sorted_words = None\n",
        "    sorted_words = sorted(word_count, key=word_count.get, reverse=True)\n",
        "    \n",
        "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
        "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word' and\n",
        "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
        "        \n",
        "    return word_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p0zOHvbb16_"
      },
      "source": [
        "word_dict = build_dict(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNhkB4Hsb30F",
        "outputId": "573b4a3b-dc7c-4f14-d5d6-4081d32835b3"
      },
      "source": [
        "count = 0\n",
        "for word in word_dict:\n",
        "    print(word)\n",
        "    if count > 4:\n",
        "        break\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "movi\n",
            "film\n",
            "like\n",
            "one\n",
            "make\n",
            "see\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-aLNMfxaM3o"
      },
      "source": [
        "## Transform the reviews\n",
        "\n",
        "Now that we have our word dictionary which allows us to transform the words appearing in the reviews into integers, it is time to make use of it and convert our reviews to their integer sequence representation, making sure to pad or truncate to a fixed length, which in our case is `500`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRvcRY0Nb9-7"
      },
      "source": [
        "def convert_and_pad(word_dict, sentence, pad=500):\n",
        "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
        "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
        "    \n",
        "    working_sentence = [NOWORD] * pad\n",
        "    \n",
        "    for word_index, word in enumerate(sentence[:pad]):\n",
        "        if word in word_dict:\n",
        "            working_sentence[word_index] = word_dict[word]\n",
        "        else:\n",
        "            working_sentence[word_index] = INFREQ\n",
        "            \n",
        "    return working_sentence, min(len(sentence), pad)\n",
        "\n",
        "def convert_and_pad_data(word_dict, data, pad=500):\n",
        "    result = []\n",
        "    lengths = []\n",
        "    \n",
        "    for sentence in data:\n",
        "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
        "        result.append(converted)\n",
        "        lengths.append(leng)\n",
        "        \n",
        "    return np.array(result), np.array(lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgPyiSJecAWP"
      },
      "source": [
        "X_train, X_train_len = convert_and_pad_data(word_dict, X_train)\n",
        "X_test, X_test_len = convert_and_pad_data(word_dict, X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "846ZAcX1cCgr",
        "outputId": "7a17664e-9bb2-4370-c8ef-21f01937c84e"
      },
      "source": [
        "# Use this cell to examine one of the processed reviews to make sure everything is working as intended.\n",
        "X_train[100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1, 2901,  597,    1,    1,  324,  135,   57,   64,  576,   31,\n",
              "         14,    9, 1104,  196,  524,   12,   67,  225,  275,  161,  888,\n",
              "         43,    2,   97,    1,  743,  736,  743,  700,  454,  147,  727,\n",
              "       1773,  467,   55,  460,   83,    1, 2563,  135,   46,    1,    1,\n",
              "        808,  199,  672,  144, 2569, 2569,  530,    1, 2252, 4061,  284,\n",
              "         73,    2,    1,    6,    1,  533,  456,    2,   48,   14,   15,\n",
              "        147,   77, 1693,    1,    2, 1067,  430,  292, 1643,    1,  135,\n",
              "       1668, 3390,  147,   20,    1, 2431,  147,    4,    1,    1,    1,\n",
              "          1, 2909,   23,  397,    1,  264,   39, 4029,  112,    1,    1,\n",
              "        224,   66,   43,  841,    1, 3923,  134, 4335,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36SGD-WRakVc"
      },
      "source": [
        "## Save the processed training dataset locally\n",
        "It is important to note the format of the data that we are saving as we will need to know it when we write the training code. In our case, each row of the dataset has the form `label`, `length`, `review[500]` where `review[500]` is a sequence of `500` integers representing the words in the review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5EoyVM8ewHW"
      },
      "source": [
        "pd.concat([pd.DataFrame(y_train), pd.DataFrame(X_train_len), pd.DataFrame(X_train)], axis=1) \\\n",
        "        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSLwlTLua23u"
      },
      "source": [
        "First we will load a small portion of the training data set to use as a sample. It would be very time consuming to try and train the model completely in the notebook as we do not have access to a powerful computing instance which inturn increases the time it takes to train the whole sample. However, we can work on a small bit of the data to get a feel for how our training script is behaving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWd6kFxOcGyq"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "\n",
        "# Read in only the first 250 rows\n",
        "train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=250)\n",
        "\n",
        "# Turn the input pandas dataframe into tensors\n",
        "train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\n",
        "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\n",
        "\n",
        "# Build the dataset\n",
        "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\n",
        "# Build the dataloader\n",
        "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXrlEloqcMhd"
      },
      "source": [
        "def train(model, train_loader, epochs, optimizer, loss_fn, device):\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:         \n",
        "            batch_X, batch_y = batch\n",
        "            \n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            out = model.forward(batch_X)\n",
        "            loss = loss_fn(out, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.data.item()\n",
        "        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DGdnRf9cObP",
        "outputId": "c9aa344b-3e3f-43a8-e0bd-30abef7b1c77"
      },
      "source": [
        "import torch.optim as optim\n",
        "from train.model import LSTMClassifier\n",
        "embedding_dim = 32\n",
        "hidden_dim = 100\n",
        "vocab_size = 5000\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LSTMClassifier(embedding_dim, hidden_dim, vocab_size).to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "\n",
        "trained_model = train(model, train_sample_dl, 30, optimizer, loss_fn, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, BCELoss: 0.6925389885902404\n",
            "Epoch: 2, BCELoss: 0.6850505590438842\n",
            "Epoch: 3, BCELoss: 0.6786375403404236\n",
            "Epoch: 4, BCELoss: 0.6711050629615783\n",
            "Epoch: 5, BCELoss: 0.6607265591621398\n",
            "Epoch: 6, BCELoss: 0.6444045662879944\n",
            "Epoch: 7, BCELoss: 0.6185956716537475\n",
            "Epoch: 8, BCELoss: 0.5854316234588623\n",
            "Epoch: 9, BCELoss: 0.5512206196784973\n",
            "Epoch: 10, BCELoss: 0.505123770236969\n",
            "Epoch: 11, BCELoss: 0.4544136881828308\n",
            "Epoch: 12, BCELoss: 0.40600094199180603\n",
            "Epoch: 13, BCELoss: 0.36829398274421693\n",
            "Epoch: 14, BCELoss: 0.33865317702293396\n",
            "Epoch: 15, BCELoss: 0.3641043663024902\n",
            "Epoch: 16, BCELoss: 0.3139317035675049\n",
            "Epoch: 17, BCELoss: 0.3262467384338379\n",
            "Epoch: 18, BCELoss: 0.28491824865341187\n",
            "Epoch: 19, BCELoss: 0.21986788809299468\n",
            "Epoch: 20, BCELoss: 0.19498106241226196\n",
            "Epoch: 21, BCELoss: 0.22358021438121795\n",
            "Epoch: 22, BCELoss: 0.17137837409973145\n",
            "Epoch: 23, BCELoss: 0.15338201224803924\n",
            "Epoch: 24, BCELoss: 0.10329500585794449\n",
            "Epoch: 25, BCELoss: 0.09225806444883347\n",
            "Epoch: 26, BCELoss: 0.0701958030462265\n",
            "Epoch: 27, BCELoss: 0.05539115965366363\n",
            "Epoch: 28, BCELoss: 0.044146285951137544\n",
            "Epoch: 29, BCELoss: 0.03426871113479137\n",
            "Epoch: 30, BCELoss: 0.0276970025151968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKJW5b4HbQpm"
      },
      "source": [
        "## Saving the trained model.\n",
        "\n",
        "After training for a while we saved our trained model including the parameters and settings. This is particularly useful for two main reasons:\n",
        "1. If it takes to much time to train our model and we want to continue training at a later time.\n",
        "2. We are particularly interested in the inference or the end result with out going through the whole preprocessing and training steps.\n",
        "\n",
        "**NOTE**: So if anyone is using this code they can start from this step and test our model using some review texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXEULs0JCe4c"
      },
      "source": [
        "checkpoint = {'model': LSTMClassifier(32, 100, 5000),\n",
        "              'state_dict': trained_model.state_dict(),\n",
        "              'optimizer' : optimizer.state_dict(),\n",
        "              'word_dict': trained_model.word_dict}\n",
        "\n",
        "\n",
        "# Save trained model and parameters.\n",
        "torch.save(checkpoint, 'spoiler_alert_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ume6KYQ2G1pR"
      },
      "source": [
        "# Load our saved model\n",
        "\n",
        "def load_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath, map_location=torch.device('cpu'))\n",
        "    model = checkpoint['model']\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    for parameter in model.parameters():\n",
        "        parameter.requires_grad = False\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnGj6FrSG2tr",
        "outputId": "c24c7c3c-59a1-4610-f5ce-b1099e8732af"
      },
      "source": [
        "model = load_checkpoint('spoiler_alert_model.pth')\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMClassifier(\n",
            "  (embedding): Embedding(5000, 32, padding_idx=0)\n",
            "  (lstm): LSTM(32, 100)\n",
            "  (dense): Linear(in_features=100, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_o7amxA4Z-N"
      },
      "source": [
        "def predict_fn(input_data, model, word_dict):\n",
        "    print('Inferring review of input data.')\n",
        "\n",
        "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    # Load the saved word_dict.\n",
        "    word_dict_path = os.path.join('/content/drive/My Drive/Colab Notebooks/nlp/data/pytorch', 'word_dict.pkl')\n",
        "    with open(word_dict_path, 'rb') as f:\n",
        "        model.word_dict = pickle.load(f)\n",
        "        \n",
        "    if model.word_dict is None:\n",
        "        raise Exception('Model has not been loaded properly, no word_dict.')\n",
        "    \n",
        "    # Process input_data so that it is ready to be sent to our model.\n",
        "    # You should produce two variables:\n",
        "    # data_X   - A sequence of length 500 which represents the converted review\n",
        "    # data_len - The length of the review\n",
        "\n",
        "    data_X, data_len = convert_and_pad(model.word_dict, review_to_words(input_data))\n",
        "\n",
        "    # Using data_X and data_len we construct an appropriate input tensor. Remember\n",
        "    # that our model expects input data of the form 'len, review[500]'.\n",
        "    data_pack = np.hstack((data_len, data_X))\n",
        "    data_pack = data_pack.reshape(1, -1)\n",
        "    \n",
        "    data = torch.from_numpy(data_pack)\n",
        "    # data = data.to(device)\n",
        "\n",
        "    # Make sure to put the model into evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Compute the result of applying the model to the input data. The variable `result` should\n",
        "    # be a numpy array which contains a single integer which is either 1 or 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.forward(data)\n",
        "\n",
        "    if np.round(out.numpy()) == 1:\n",
        "      result = \"Spoiler!\"\n",
        "    else:\n",
        "      result = \"Not Spoiler!\"    \n",
        "    # result = np.round(out.numpy())\n",
        "\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYG45CYZHITe"
      },
      "source": [
        "test_review = \"Call this 1999 teenage version of Pulp Fiction whatever you want but please don't call it original. This is one of the worst Tarantino rip-offs I have ever seen. The first two stories are mindless and un-entertaining. The only redeeming value the movie had was some of the humor contained in the last story. I noticed that a lot of IMDB users have complemented the acting in this film; I have to disagree. The acting, especially among some of the younger characters, was either dry, silly, or overdone. 3/10\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtEziimXHE5M",
        "outputId": "498587eb-3756-4aa3-87e4-6c44a3b2515b"
      },
      "source": [
        "print(predict_fn(test_review, model, word_dict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inferring review of input data.\n",
            "Not Spoiler!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}